{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, TimeDistributed, Dense, Activation, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../Data/\"\n",
    "data_file = \"Data_Tunes.txt\"\n",
    "charIndex_json = \"char_to_index.json\"\n",
    "model_weights_directory = '../Data/Model_Weights/'\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batches(all_chars, unique_chars):\n",
    "    length = all_chars.shape[0]\n",
    "    batch_chars = int(length / BATCH_SIZE) #155222/16 = 9701\n",
    "    \n",
    "    for start in range(0, batch_chars - SEQ_LENGTH, 64):  #(0, 9637, 64)  #it denotes number of batches. It runs everytime when\n",
    "        #new batch is created. We have a total of 151 batches.\n",
    "        X = np.zeros((BATCH_SIZE, SEQ_LENGTH))    #(16, 64)\n",
    "        Y = np.zeros((BATCH_SIZE, SEQ_LENGTH, unique_chars))   #(16, 64, 87)\n",
    "        for batch_index in range(0, 16):  #it denotes each row in a batch.  \n",
    "            for i in range(0, 64):  #it denotes each column in a batch. Each column represents each character means \n",
    "                #each time-step character in a sequence.\n",
    "                X[batch_index, i] = all_chars[batch_index * batch_chars + start + i]\n",
    "                Y[batch_index, i, all_chars[batch_index * batch_chars + start + i + 1]] = 1 #here we have added '1' because the\n",
    "                #correct label will be the next character in the sequence. So, the next character will be denoted by\n",
    "                #all_chars[batch_index * batch_chars + start + i + 1]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def built_model(batch_size, seq_length, unique_chars):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim = unique_chars, output_dim = 512, batch_input_shape = (batch_size, seq_length))) \n",
    "    \n",
    "    model.add(LSTM(256, return_sequences = True, stateful = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(128, return_sequences = True, stateful = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(TimeDistributed(Dense(unique_chars)))\n",
    "\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(data, epochs = 80):\n",
    "    #mapping character to index\n",
    "    char_to_index = {ch: i for (i, ch) in enumerate(sorted(list(set(data))))}\n",
    "    print(\"Number of unique characters in our whole tunes database = {}\".format(len(char_to_index))) #87\n",
    "    \n",
    "    with open(os.path.join(data_directory, charIndex_json), mode = \"w\") as f:\n",
    "        json.dump(char_to_index, f)\n",
    "        \n",
    "    index_to_char = {i: ch for (ch, i) in char_to_index.items()}\n",
    "    unique_chars = len(char_to_index)\n",
    "    \n",
    "    model = built_model(BATCH_SIZE, SEQ_LENGTH, unique_chars)\n",
    "    model.summary()\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "    \n",
    "    all_characters = np.asarray([char_to_index[c] for c in data], dtype = np.int32)\n",
    "    print(\"Total number of characters = \"+str(all_characters.shape[0])) #155222\n",
    "    \n",
    "    epoch_number, loss, accuracy = [], [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
    "        final_epoch_loss, final_epoch_accuracy = 0, 0\n",
    "        epoch_number.append(epoch+1)\n",
    "        \n",
    "        for i, (x, y) in enumerate(read_batches(all_characters, unique_chars)):\n",
    "            final_epoch_loss, final_epoch_accuracy = model.train_on_batch(x, y) #check documentation of train_on_batch here: https://keras.io/models/sequential/\n",
    "            print(\"Batch: {}, Loss: {}, Accuracy: {}\".format(i+1, final_epoch_loss, final_epoch_accuracy))\n",
    "            #here, above we are reading the batches one-by-one and train our model on each batch one-by-one.\n",
    "        loss.append(final_epoch_loss)\n",
    "        accuracy.append(final_epoch_accuracy)\n",
    "        \n",
    "        #saving weights after every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            if not os.path.exists(model_weights_directory):\n",
    "                os.makedirs(model_weights_directory)\n",
    "            model.save_weights(os.path.join(model_weights_directory, \"Weights_{}.h5\".format(epoch+1)))\n",
    "            print('Saved Weights at epoch {} to file Weights_{}.h5'.format(epoch+1, epoch+1))\n",
    "    \n",
    "    #creating dataframe and record all the losses and accuracies at each epoch\n",
    "    log_frame = pd.DataFrame(columns = [\"Epoch\", \"Loss\", \"Accuracy\"])\n",
    "    log_frame[\"Epoch\"] = epoch_number\n",
    "    log_frame[\"Loss\"] = loss\n",
    "    log_frame[\"Accuracy\"] = accuracy\n",
    "    log_frame.to_csv(\"../Data/log.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters in our whole tunes database = 87\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (16, 64, 512)             44544     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (16, 64, 256)             787456    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (16, 64, 256)             0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (16, 64, 128)             197120    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (16, 64, 128)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (16, 64, 87)              11223     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (16, 64, 87)              0         \n",
      "=================================================================\n",
      "Total params: 1,040,343\n",
      "Trainable params: 1,040,343\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Total number of characters = 155222\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1, Loss: 4.465232849121094, Accuracy: 0.0068359375\n",
      "Batch: 2, Loss: 4.438962459564209, Accuracy: 0.173828125\n",
      "Batch: 3, Loss: 4.410113334655762, Accuracy: 0.134765625\n",
      "Batch: 4, Loss: 4.36024808883667, Accuracy: 0.1025390625\n",
      "Batch: 5, Loss: 4.114752292633057, Accuracy: 0.1435546875\n",
      "Batch: 6, Loss: 3.8069310188293457, Accuracy: 0.166015625\n",
      "Batch: 7, Loss: 3.7187938690185547, Accuracy: 0.1630859375\n",
      "Batch: 8, Loss: 3.7469322681427, Accuracy: 0.1416015625\n",
      "Batch: 9, Loss: 3.753999948501587, Accuracy: 0.1357421875\n",
      "Batch: 10, Loss: 3.603203773498535, Accuracy: 0.1328125\n",
      "Batch: 11, Loss: 3.3470730781555176, Accuracy: 0.1455078125\n",
      "Batch: 12, Loss: 3.5461196899414062, Accuracy: 0.138671875\n",
      "Batch: 13, Loss: 3.7733259201049805, Accuracy: 0.11328125\n",
      "Batch: 14, Loss: 3.4946980476379395, Accuracy: 0.1298828125\n",
      "Batch: 15, Loss: 3.735079765319824, Accuracy: 0.115234375\n",
      "Batch: 16, Loss: 3.4264934062957764, Accuracy: 0.1572265625\n",
      "Batch: 17, Loss: 3.3588309288024902, Accuracy: 0.1767578125\n",
      "Batch: 18, Loss: 3.324333667755127, Accuracy: 0.1669921875\n",
      "Batch: 19, Loss: 3.595219850540161, Accuracy: 0.125\n",
      "Batch: 20, Loss: 3.7246878147125244, Accuracy: 0.115234375\n",
      "Batch: 21, Loss: 3.5501251220703125, Accuracy: 0.1298828125\n",
      "Batch: 22, Loss: 3.298093557357788, Accuracy: 0.1611328125\n",
      "Batch: 23, Loss: 3.4221110343933105, Accuracy: 0.1396484375\n",
      "Batch: 24, Loss: 3.5822877883911133, Accuracy: 0.1220703125\n",
      "Batch: 25, Loss: 3.4918932914733887, Accuracy: 0.12890625\n",
      "Batch: 26, Loss: 3.4609298706054688, Accuracy: 0.13671875\n",
      "Batch: 27, Loss: 3.4305105209350586, Accuracy: 0.1298828125\n",
      "Batch: 28, Loss: 3.280673027038574, Accuracy: 0.1513671875\n",
      "Batch: 29, Loss: 3.4827566146850586, Accuracy: 0.1337890625\n",
      "Batch: 30, Loss: 3.778752326965332, Accuracy: 0.09375\n",
      "Batch: 31, Loss: 3.6755058765411377, Accuracy: 0.1201171875\n",
      "Batch: 32, Loss: 3.439270496368408, Accuracy: 0.1201171875\n",
      "Batch: 33, Loss: 3.4481594562530518, Accuracy: 0.1455078125\n",
      "Batch: 34, Loss: 3.4036242961883545, Accuracy: 0.1455078125\n",
      "Batch: 35, Loss: 3.490952253341675, Accuracy: 0.119140625\n",
      "Batch: 36, Loss: 3.6394290924072266, Accuracy: 0.1015625\n",
      "Batch: 37, Loss: 3.4917774200439453, Accuracy: 0.1181640625\n",
      "Batch: 38, Loss: 3.406435966491699, Accuracy: 0.12890625\n",
      "Batch: 39, Loss: 3.4774580001831055, Accuracy: 0.134765625\n",
      "Batch: 40, Loss: 3.5961475372314453, Accuracy: 0.1162109375\n",
      "Batch: 41, Loss: 3.588775157928467, Accuracy: 0.1220703125\n",
      "Batch: 42, Loss: 3.4227919578552246, Accuracy: 0.1435546875\n",
      "Batch: 43, Loss: 3.3018903732299805, Accuracy: 0.1630859375\n",
      "Batch: 44, Loss: 3.313779354095459, Accuracy: 0.1552734375\n",
      "Batch: 45, Loss: 3.3213820457458496, Accuracy: 0.15625\n",
      "Batch: 46, Loss: 3.6386373043060303, Accuracy: 0.1123046875\n",
      "Batch: 47, Loss: 3.6324868202209473, Accuracy: 0.1123046875\n",
      "Batch: 48, Loss: 3.413224697113037, Accuracy: 0.14453125\n",
      "Batch: 49, Loss: 3.3688535690307617, Accuracy: 0.1376953125\n",
      "Batch: 50, Loss: 3.340453624725342, Accuracy: 0.14453125\n",
      "Batch: 51, Loss: 3.334853410720825, Accuracy: 0.140625\n",
      "Batch: 52, Loss: 3.4149398803710938, Accuracy: 0.1298828125\n",
      "Batch: 53, Loss: 3.3748483657836914, Accuracy: 0.1337890625\n",
      "Batch: 54, Loss: 3.4326109886169434, Accuracy: 0.1376953125\n",
      "Batch: 55, Loss: 3.331822395324707, Accuracy: 0.154296875\n",
      "Batch: 56, Loss: 3.391186237335205, Accuracy: 0.16015625\n",
      "Batch: 57, Loss: 3.398153305053711, Accuracy: 0.1416015625\n",
      "Batch: 58, Loss: 3.321455955505371, Accuracy: 0.1484375\n",
      "Batch: 59, Loss: 3.5275096893310547, Accuracy: 0.1318359375\n",
      "Batch: 60, Loss: 3.3303732872009277, Accuracy: 0.1455078125\n",
      "Batch: 61, Loss: 3.3174333572387695, Accuracy: 0.154296875\n",
      "Batch: 62, Loss: 3.41711163520813, Accuracy: 0.1474609375\n",
      "Batch: 63, Loss: 3.351266622543335, Accuracy: 0.1552734375\n",
      "Batch: 64, Loss: 3.37096905708313, Accuracy: 0.1474609375\n",
      "Batch: 65, Loss: 3.3615241050720215, Accuracy: 0.1533203125\n",
      "Batch: 66, Loss: 3.3203725814819336, Accuracy: 0.16015625\n",
      "Batch: 67, Loss: 3.1830968856811523, Accuracy: 0.1787109375\n",
      "Batch: 68, Loss: 3.246035575866699, Accuracy: 0.1826171875\n",
      "Batch: 69, Loss: 3.357161521911621, Accuracy: 0.1513671875\n",
      "Batch: 70, Loss: 3.318653106689453, Accuracy: 0.16796875\n",
      "Batch: 71, Loss: 3.2402608394622803, Accuracy: 0.1728515625\n",
      "Batch: 72, Loss: 3.2700388431549072, Accuracy: 0.1630859375\n",
      "Batch: 73, Loss: 3.359208583831787, Accuracy: 0.1484375\n",
      "Batch: 74, Loss: 3.309929847717285, Accuracy: 0.1533203125\n",
      "Batch: 75, Loss: 3.1875884532928467, Accuracy: 0.1708984375\n",
      "Batch: 76, Loss: 3.0906753540039062, Accuracy: 0.181640625\n",
      "Batch: 77, Loss: 3.166329860687256, Accuracy: 0.17578125\n",
      "Batch: 78, Loss: 3.4120330810546875, Accuracy: 0.13671875\n",
      "Batch: 79, Loss: 3.397036075592041, Accuracy: 0.14453125\n",
      "Batch: 80, Loss: 3.046637773513794, Accuracy: 0.1953125\n",
      "Batch: 81, Loss: 2.9211013317108154, Accuracy: 0.208984375\n",
      "Batch: 82, Loss: 3.0212366580963135, Accuracy: 0.201171875\n",
      "Batch: 83, Loss: 3.1465601921081543, Accuracy: 0.1826171875\n",
      "Batch: 84, Loss: 3.204123020172119, Accuracy: 0.1845703125\n",
      "Batch: 85, Loss: 3.1649701595306396, Accuracy: 0.1708984375\n",
      "Batch: 86, Loss: 3.023507595062256, Accuracy: 0.1904296875\n",
      "Batch: 87, Loss: 3.0790600776672363, Accuracy: 0.201171875\n",
      "Batch: 88, Loss: 3.077388286590576, Accuracy: 0.205078125\n",
      "Batch: 89, Loss: 3.0649991035461426, Accuracy: 0.20703125\n",
      "Batch: 90, Loss: 3.1045913696289062, Accuracy: 0.2158203125\n",
      "Batch: 91, Loss: 3.0745134353637695, Accuracy: 0.193359375\n",
      "Batch: 92, Loss: 3.0038983821868896, Accuracy: 0.21484375\n",
      "Batch: 93, Loss: 3.0120866298675537, Accuracy: 0.2041015625\n",
      "Batch: 94, Loss: 2.9541420936584473, Accuracy: 0.2333984375\n",
      "Batch: 95, Loss: 2.7668588161468506, Accuracy: 0.2568359375\n",
      "Batch: 96, Loss: 3.0408430099487305, Accuracy: 0.2255859375\n",
      "Batch: 97, Loss: 3.0156352519989014, Accuracy: 0.2138671875\n",
      "Batch: 98, Loss: 3.0381205081939697, Accuracy: 0.2216796875\n",
      "Batch: 99, Loss: 2.8401436805725098, Accuracy: 0.2470703125\n",
      "Batch: 100, Loss: 2.7307684421539307, Accuracy: 0.271484375\n",
      "Batch: 101, Loss: 2.8364205360412598, Accuracy: 0.2529296875\n",
      "Batch: 102, Loss: 2.8414177894592285, Accuracy: 0.2646484375\n",
      "Batch: 103, Loss: 3.071666717529297, Accuracy: 0.201171875\n",
      "Batch: 104, Loss: 2.8379831314086914, Accuracy: 0.25\n",
      "Batch: 105, Loss: 2.796720266342163, Accuracy: 0.2626953125\n",
      "Batch: 106, Loss: 2.905616521835327, Accuracy: 0.2265625\n",
      "Batch: 107, Loss: 2.929903030395508, Accuracy: 0.228515625\n",
      "Batch: 108, Loss: 2.9044272899627686, Accuracy: 0.25\n",
      "Batch: 109, Loss: 2.805302381515503, Accuracy: 0.271484375\n",
      "Batch: 110, Loss: 2.8000683784484863, Accuracy: 0.2509765625\n",
      "Batch: 111, Loss: 2.686519145965576, Accuracy: 0.2646484375\n",
      "Batch: 112, Loss: 2.8832435607910156, Accuracy: 0.2392578125\n",
      "Batch: 113, Loss: 2.919304847717285, Accuracy: 0.255859375\n",
      "Batch: 114, Loss: 2.6900715827941895, Accuracy: 0.28125\n",
      "Batch: 115, Loss: 2.77836537361145, Accuracy: 0.2705078125\n",
      "Batch: 116, Loss: 2.807608127593994, Accuracy: 0.267578125\n",
      "Batch: 117, Loss: 2.960247755050659, Accuracy: 0.2373046875\n",
      "Batch: 118, Loss: 2.9088175296783447, Accuracy: 0.228515625\n",
      "Batch: 119, Loss: 2.903881072998047, Accuracy: 0.2421875\n",
      "Batch: 120, Loss: 2.699110746383667, Accuracy: 0.275390625\n",
      "Batch: 121, Loss: 2.700721263885498, Accuracy: 0.29296875\n",
      "Batch: 122, Loss: 2.946995258331299, Accuracy: 0.23046875\n",
      "Batch: 123, Loss: 2.8385109901428223, Accuracy: 0.2529296875\n",
      "Batch: 124, Loss: 2.8091580867767334, Accuracy: 0.248046875\n",
      "Batch: 125, Loss: 2.7234597206115723, Accuracy: 0.2783203125\n",
      "Batch: 126, Loss: 2.6890792846679688, Accuracy: 0.2568359375\n",
      "Batch: 127, Loss: 2.8318824768066406, Accuracy: 0.2509765625\n",
      "Batch: 128, Loss: 2.797276020050049, Accuracy: 0.2626953125\n",
      "Batch: 129, Loss: 2.727360725402832, Accuracy: 0.27734375\n",
      "Batch: 130, Loss: 2.7356319427490234, Accuracy: 0.2841796875\n",
      "Batch: 131, Loss: 2.751852035522461, Accuracy: 0.2626953125\n",
      "Batch: 132, Loss: 2.805619239807129, Accuracy: 0.2509765625\n",
      "Batch: 133, Loss: 2.7054038047790527, Accuracy: 0.2841796875\n",
      "Batch: 134, Loss: 2.6696290969848633, Accuracy: 0.2734375\n",
      "Batch: 135, Loss: 2.5973055362701416, Accuracy: 0.29296875\n",
      "Batch: 136, Loss: 2.5646800994873047, Accuracy: 0.3076171875\n",
      "Batch: 137, Loss: 2.4035065174102783, Accuracy: 0.345703125\n",
      "Batch: 138, Loss: 2.5023200511932373, Accuracy: 0.3232421875\n",
      "Batch: 139, Loss: 2.493880033493042, Accuracy: 0.3125\n",
      "Batch: 140, Loss: 2.606592893600464, Accuracy: 0.298828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 141, Loss: 2.590959072113037, Accuracy: 0.314453125\n",
      "Batch: 142, Loss: 2.532702684402466, Accuracy: 0.322265625\n",
      "Batch: 143, Loss: 2.6209139823913574, Accuracy: 0.3017578125\n",
      "Batch: 144, Loss: 2.5920519828796387, Accuracy: 0.3125\n",
      "Batch: 145, Loss: 2.514592170715332, Accuracy: 0.328125\n",
      "Batch: 146, Loss: 2.6072092056274414, Accuracy: 0.3037109375\n",
      "Batch: 147, Loss: 2.5779454708099365, Accuracy: 0.3076171875\n",
      "Batch: 148, Loss: 2.532719135284424, Accuracy: 0.3134765625\n",
      "Batch: 149, Loss: 2.584620952606201, Accuracy: 0.2958984375\n",
      "Batch: 150, Loss: 2.503286838531494, Accuracy: 0.318359375\n",
      "Batch: 151, Loss: 2.6077685356140137, Accuracy: 0.3173828125\n",
      "Epoch 2/80\n",
      "Batch: 1, Loss: 2.4259793758392334, Accuracy: 0.333984375\n",
      "Batch: 2, Loss: 2.297786235809326, Accuracy: 0.34765625\n",
      "Batch: 3, Loss: 2.5091071128845215, Accuracy: 0.328125\n",
      "Batch: 4, Loss: 2.5784358978271484, Accuracy: 0.3115234375\n",
      "Batch: 5, Loss: 2.4553093910217285, Accuracy: 0.3486328125\n",
      "Batch: 6, Loss: 2.3079426288604736, Accuracy: 0.3564453125\n",
      "Batch: 7, Loss: 2.2812705039978027, Accuracy: 0.3662109375\n",
      "Batch: 8, Loss: 2.341003656387329, Accuracy: 0.353515625\n",
      "Batch: 9, Loss: 2.3921425342559814, Accuracy: 0.365234375\n",
      "Batch: 10, Loss: 2.369671583175659, Accuracy: 0.3603515625\n",
      "Batch: 11, Loss: 2.207416296005249, Accuracy: 0.388671875\n",
      "Batch: 12, Loss: 2.404292345046997, Accuracy: 0.333984375\n",
      "Batch: 13, Loss: 2.4290194511413574, Accuracy: 0.345703125\n",
      "Batch: 14, Loss: 2.3622560501098633, Accuracy: 0.34765625\n",
      "Batch: 15, Loss: 2.511608362197876, Accuracy: 0.33203125\n",
      "Batch: 16, Loss: 2.280320882797241, Accuracy: 0.3583984375\n",
      "Batch: 17, Loss: 2.2896463871002197, Accuracy: 0.3564453125\n",
      "Batch: 18, Loss: 2.2627458572387695, Accuracy: 0.369140625\n",
      "Batch: 19, Loss: 2.412707805633545, Accuracy: 0.3466796875\n",
      "Batch: 20, Loss: 2.478756904602051, Accuracy: 0.345703125\n",
      "Batch: 21, Loss: 2.2919387817382812, Accuracy: 0.380859375\n",
      "Batch: 22, Loss: 2.299959421157837, Accuracy: 0.375\n",
      "Batch: 23, Loss: 2.2353978157043457, Accuracy: 0.359375\n",
      "Batch: 24, Loss: 2.4248437881469727, Accuracy: 0.3388671875\n",
      "Batch: 25, Loss: 2.2840993404388428, Accuracy: 0.376953125\n",
      "Batch: 26, Loss: 2.1808645725250244, Accuracy: 0.396484375\n",
      "Batch: 27, Loss: 2.296600341796875, Accuracy: 0.3505859375\n",
      "Batch: 28, Loss: 2.2157890796661377, Accuracy: 0.3720703125\n",
      "Batch: 29, Loss: 2.2898926734924316, Accuracy: 0.35546875\n",
      "Batch: 30, Loss: 2.474146604537964, Accuracy: 0.3486328125\n",
      "Batch: 31, Loss: 2.446180820465088, Accuracy: 0.3662109375\n",
      "Batch: 32, Loss: 2.227166175842285, Accuracy: 0.3916015625\n",
      "Batch: 33, Loss: 2.3093998432159424, Accuracy: 0.3701171875\n",
      "Batch: 34, Loss: 2.355635166168213, Accuracy: 0.3701171875\n",
      "Batch: 35, Loss: 2.322199821472168, Accuracy: 0.365234375\n",
      "Batch: 36, Loss: 2.418336868286133, Accuracy: 0.359375\n",
      "Batch: 37, Loss: 2.320826768875122, Accuracy: 0.376953125\n",
      "Batch: 38, Loss: 2.2517495155334473, Accuracy: 0.37109375\n",
      "Batch: 39, Loss: 2.3165698051452637, Accuracy: 0.3828125\n",
      "Batch: 40, Loss: 2.393843650817871, Accuracy: 0.369140625\n",
      "Batch: 41, Loss: 2.283416748046875, Accuracy: 0.39453125\n",
      "Batch: 42, Loss: 2.082655906677246, Accuracy: 0.4267578125\n",
      "Batch: 43, Loss: 2.0685184001922607, Accuracy: 0.412109375\n",
      "Batch: 44, Loss: 2.037478446960449, Accuracy: 0.4384765625\n",
      "Batch: 45, Loss: 2.064765453338623, Accuracy: 0.4345703125\n",
      "Batch: 46, Loss: 2.288133144378662, Accuracy: 0.40234375\n",
      "Batch: 47, Loss: 2.3549485206604004, Accuracy: 0.3857421875\n",
      "Batch: 48, Loss: 2.2368812561035156, Accuracy: 0.392578125\n",
      "Batch: 49, Loss: 2.2521939277648926, Accuracy: 0.3759765625\n",
      "Batch: 50, Loss: 2.223113536834717, Accuracy: 0.3876953125\n",
      "Batch: 51, Loss: 2.2444870471954346, Accuracy: 0.375\n",
      "Batch: 52, Loss: 2.2619056701660156, Accuracy: 0.3994140625\n",
      "Batch: 53, Loss: 2.0714080333709717, Accuracy: 0.4345703125\n",
      "Batch: 54, Loss: 2.194930076599121, Accuracy: 0.416015625\n",
      "Batch: 55, Loss: 2.0523810386657715, Accuracy: 0.44140625\n",
      "Batch: 56, Loss: 2.19381046295166, Accuracy: 0.4111328125\n",
      "Batch: 57, Loss: 2.147674083709717, Accuracy: 0.4189453125\n",
      "Batch: 58, Loss: 2.14577579498291, Accuracy: 0.4267578125\n",
      "Batch: 59, Loss: 2.1077990531921387, Accuracy: 0.439453125\n",
      "Batch: 60, Loss: 2.060573101043701, Accuracy: 0.4375\n",
      "Batch: 61, Loss: 2.0798025131225586, Accuracy: 0.431640625\n",
      "Batch: 62, Loss: 2.2074413299560547, Accuracy: 0.404296875\n",
      "Batch: 63, Loss: 2.120210647583008, Accuracy: 0.421875\n",
      "Batch: 64, Loss: 2.10894775390625, Accuracy: 0.4287109375\n",
      "Batch: 65, Loss: 2.1302363872528076, Accuracy: 0.408203125\n",
      "Batch: 66, Loss: 2.024148941040039, Accuracy: 0.4619140625\n",
      "Batch: 67, Loss: 2.057002544403076, Accuracy: 0.4501953125\n",
      "Batch: 68, Loss: 2.183676242828369, Accuracy: 0.4287109375\n",
      "Batch: 69, Loss: 2.1412315368652344, Accuracy: 0.439453125\n",
      "Batch: 70, Loss: 2.1776986122131348, Accuracy: 0.423828125\n",
      "Batch: 71, Loss: 2.0632734298706055, Accuracy: 0.4404296875\n",
      "Batch: 72, Loss: 2.061056613922119, Accuracy: 0.44921875\n",
      "Batch: 73, Loss: 2.195875883102417, Accuracy: 0.41015625\n",
      "Batch: 74, Loss: 2.0941951274871826, Accuracy: 0.421875\n",
      "Batch: 75, Loss: 1.9914339780807495, Accuracy: 0.4619140625\n",
      "Batch: 76, Loss: 2.013104200363159, Accuracy: 0.427734375\n",
      "Batch: 77, Loss: 2.091322183609009, Accuracy: 0.4150390625\n",
      "Batch: 78, Loss: 2.1914448738098145, Accuracy: 0.427734375\n",
      "Batch: 79, Loss: 2.068495988845825, Accuracy: 0.4775390625\n",
      "Batch: 80, Loss: 1.923085331916809, Accuracy: 0.4716796875\n",
      "Batch: 81, Loss: 1.9475406408309937, Accuracy: 0.4462890625\n",
      "Batch: 82, Loss: 1.9392049312591553, Accuracy: 0.4423828125\n",
      "Batch: 83, Loss: 1.965198040008545, Accuracy: 0.4541015625\n",
      "Batch: 84, Loss: 2.0070199966430664, Accuracy: 0.4814453125\n",
      "Batch: 85, Loss: 1.963822603225708, Accuracy: 0.4775390625\n",
      "Batch: 86, Loss: 2.0504231452941895, Accuracy: 0.4296875\n",
      "Batch: 87, Loss: 2.0217766761779785, Accuracy: 0.4736328125\n",
      "Batch: 88, Loss: 2.057361602783203, Accuracy: 0.44921875\n",
      "Batch: 89, Loss: 2.051661491394043, Accuracy: 0.451171875\n",
      "Batch: 90, Loss: 1.9751300811767578, Accuracy: 0.4599609375\n",
      "Batch: 91, Loss: 1.97901451587677, Accuracy: 0.455078125\n",
      "Batch: 92, Loss: 2.0345406532287598, Accuracy: 0.455078125\n",
      "Batch: 93, Loss: 1.9360905885696411, Accuracy: 0.474609375\n",
      "Batch: 94, Loss: 1.9237407445907593, Accuracy: 0.46484375\n",
      "Batch: 95, Loss: 1.8778921365737915, Accuracy: 0.4697265625\n",
      "Batch: 96, Loss: 2.0336618423461914, Accuracy: 0.462890625\n",
      "Batch: 97, Loss: 1.9375455379486084, Accuracy: 0.48046875\n",
      "Batch: 98, Loss: 1.8781394958496094, Accuracy: 0.52734375\n",
      "Batch: 99, Loss: 1.8653490543365479, Accuracy: 0.482421875\n",
      "Batch: 100, Loss: 1.8052964210510254, Accuracy: 0.490234375\n",
      "Batch: 101, Loss: 1.8875720500946045, Accuracy: 0.4697265625\n",
      "Batch: 102, Loss: 1.8300354480743408, Accuracy: 0.5048828125\n",
      "Batch: 103, Loss: 2.019015312194824, Accuracy: 0.4921875\n",
      "Batch: 104, Loss: 1.8131797313690186, Accuracy: 0.515625\n",
      "Batch: 105, Loss: 1.9142929315567017, Accuracy: 0.4736328125\n",
      "Batch: 106, Loss: 1.9592373371124268, Accuracy: 0.4765625\n",
      "Batch: 107, Loss: 2.0279040336608887, Accuracy: 0.4619140625\n",
      "Batch: 108, Loss: 2.0839896202087402, Accuracy: 0.466796875\n",
      "Batch: 109, Loss: 2.076784372329712, Accuracy: 0.458984375\n",
      "Batch: 110, Loss: 1.7876098155975342, Accuracy: 0.5048828125\n",
      "Batch: 111, Loss: 1.8640117645263672, Accuracy: 0.4853515625\n",
      "Batch: 112, Loss: 1.998689889907837, Accuracy: 0.4697265625\n",
      "Batch: 113, Loss: 1.9994124174118042, Accuracy: 0.4638671875\n",
      "Batch: 114, Loss: 1.9548965692520142, Accuracy: 0.455078125\n",
      "Batch: 115, Loss: 2.012596607208252, Accuracy: 0.4814453125\n",
      "Batch: 116, Loss: 2.0265989303588867, Accuracy: 0.4443359375\n",
      "Batch: 117, Loss: 2.0152528285980225, Accuracy: 0.478515625\n",
      "Batch: 118, Loss: 1.9109019041061401, Accuracy: 0.5009765625\n",
      "Batch: 119, Loss: 1.933097004890442, Accuracy: 0.4990234375\n",
      "Batch: 120, Loss: 1.9177813529968262, Accuracy: 0.4697265625\n",
      "Batch: 121, Loss: 1.9698219299316406, Accuracy: 0.470703125\n",
      "Batch: 122, Loss: 1.939676284790039, Accuracy: 0.4775390625\n",
      "Batch: 123, Loss: 1.9210195541381836, Accuracy: 0.4931640625\n",
      "Batch: 124, Loss: 1.945472240447998, Accuracy: 0.4892578125\n",
      "Batch: 125, Loss: 1.9654889106750488, Accuracy: 0.4580078125\n",
      "Batch: 126, Loss: 1.9346983432769775, Accuracy: 0.4541015625\n",
      "Batch: 127, Loss: 1.8730018138885498, Accuracy: 0.498046875\n",
      "Batch: 128, Loss: 2.024991512298584, Accuracy: 0.4619140625\n",
      "Batch: 129, Loss: 1.9054681062698364, Accuracy: 0.478515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 130, Loss: 2.074617385864258, Accuracy: 0.4521484375\n",
      "Batch: 131, Loss: 1.9177411794662476, Accuracy: 0.470703125\n",
      "Batch: 132, Loss: 1.9631651639938354, Accuracy: 0.4755859375\n",
      "Batch: 133, Loss: 1.9061217308044434, Accuracy: 0.478515625\n",
      "Batch: 134, Loss: 1.877782940864563, Accuracy: 0.4814453125\n",
      "Batch: 135, Loss: 1.8094730377197266, Accuracy: 0.517578125\n",
      "Batch: 136, Loss: 1.8410136699676514, Accuracy: 0.4833984375\n",
      "Batch: 137, Loss: 1.7223100662231445, Accuracy: 0.48828125\n",
      "Batch: 138, Loss: 1.6585032939910889, Accuracy: 0.53515625\n",
      "Batch: 139, Loss: 1.7189562320709229, Accuracy: 0.50390625\n",
      "Batch: 140, Loss: 1.873392105102539, Accuracy: 0.4892578125\n",
      "Batch: 141, Loss: 1.8363512754440308, Accuracy: 0.5068359375\n",
      "Batch: 142, Loss: 1.8479151725769043, Accuracy: 0.4892578125\n",
      "Batch: 143, Loss: 1.9093008041381836, Accuracy: 0.4716796875\n",
      "Batch: 144, Loss: 1.836427927017212, Accuracy: 0.5068359375\n",
      "Batch: 145, Loss: 1.75357985496521, Accuracy: 0.4990234375\n",
      "Batch: 146, Loss: 1.893491506576538, Accuracy: 0.46484375\n",
      "Batch: 147, Loss: 1.8136448860168457, Accuracy: 0.4990234375\n",
      "Batch: 148, Loss: 1.941643476486206, Accuracy: 0.4423828125\n",
      "Batch: 149, Loss: 1.866485357284546, Accuracy: 0.4638671875\n",
      "Batch: 150, Loss: 1.8057518005371094, Accuracy: 0.4873046875\n",
      "Batch: 151, Loss: 1.8105170726776123, Accuracy: 0.5146484375\n",
      "Epoch 3/80\n",
      "Batch: 1, Loss: 1.9329116344451904, Accuracy: 0.4345703125\n",
      "Batch: 2, Loss: 1.7068116664886475, Accuracy: 0.494140625\n",
      "Batch: 3, Loss: 1.772562026977539, Accuracy: 0.5048828125\n",
      "Batch: 4, Loss: 1.7288445234298706, Accuracy: 0.541015625\n",
      "Batch: 5, Loss: 1.7571908235549927, Accuracy: 0.517578125\n",
      "Batch: 6, Loss: 1.7424757480621338, Accuracy: 0.4873046875\n",
      "Batch: 7, Loss: 1.6931285858154297, Accuracy: 0.50390625\n",
      "Batch: 8, Loss: 1.671598196029663, Accuracy: 0.5205078125\n",
      "Batch: 9, Loss: 1.6626579761505127, Accuracy: 0.529296875\n",
      "Batch: 10, Loss: 1.7481298446655273, Accuracy: 0.4970703125\n",
      "Batch: 11, Loss: 1.7416847944259644, Accuracy: 0.4755859375\n",
      "Batch: 12, Loss: 1.8374069929122925, Accuracy: 0.4814453125\n",
      "Batch: 13, Loss: 1.6454373598098755, Accuracy: 0.5458984375\n",
      "Batch: 14, Loss: 1.7987772226333618, Accuracy: 0.4951171875\n",
      "Batch: 15, Loss: 1.7830252647399902, Accuracy: 0.5166015625\n",
      "Batch: 16, Loss: 1.7009296417236328, Accuracy: 0.5078125\n",
      "Batch: 17, Loss: 1.74193274974823, Accuracy: 0.482421875\n",
      "Batch: 18, Loss: 1.7877254486083984, Accuracy: 0.4716796875\n",
      "Batch: 19, Loss: 1.8129019737243652, Accuracy: 0.4873046875\n",
      "Batch: 20, Loss: 1.8062596321105957, Accuracy: 0.5166015625\n",
      "Batch: 21, Loss: 1.6652333736419678, Accuracy: 0.54296875\n",
      "Batch: 22, Loss: 1.8290386199951172, Accuracy: 0.4765625\n",
      "Batch: 23, Loss: 1.6645913124084473, Accuracy: 0.5126953125\n",
      "Batch: 24, Loss: 1.7697117328643799, Accuracy: 0.4921875\n",
      "Batch: 25, Loss: 1.6897752285003662, Accuracy: 0.494140625\n",
      "Batch: 26, Loss: 1.570554256439209, Accuracy: 0.5556640625\n",
      "Batch: 27, Loss: 1.7184584140777588, Accuracy: 0.490234375\n",
      "Batch: 28, Loss: 1.705561876296997, Accuracy: 0.494140625\n",
      "Batch: 29, Loss: 1.7395775318145752, Accuracy: 0.4892578125\n",
      "Batch: 30, Loss: 1.7810803651809692, Accuracy: 0.5087890625\n",
      "Batch: 31, Loss: 1.7432622909545898, Accuracy: 0.5322265625\n",
      "Batch: 32, Loss: 1.6350059509277344, Accuracy: 0.5302734375\n",
      "Batch: 33, Loss: 1.8231666088104248, Accuracy: 0.4765625\n",
      "Batch: 34, Loss: 1.8912873268127441, Accuracy: 0.46484375\n",
      "Batch: 35, Loss: 1.7705039978027344, Accuracy: 0.4931640625\n",
      "Batch: 36, Loss: 1.770716905593872, Accuracy: 0.5068359375\n",
      "Batch: 37, Loss: 1.782084345817566, Accuracy: 0.501953125\n",
      "Batch: 38, Loss: 1.740288257598877, Accuracy: 0.4873046875\n",
      "Batch: 39, Loss: 1.759210467338562, Accuracy: 0.486328125\n",
      "Batch: 40, Loss: 1.819071650505066, Accuracy: 0.5107421875\n",
      "Batch: 41, Loss: 1.7967387437820435, Accuracy: 0.525390625\n",
      "Batch: 42, Loss: 1.5413787364959717, Accuracy: 0.552734375\n",
      "Batch: 43, Loss: 1.6477022171020508, Accuracy: 0.505859375\n",
      "Batch: 44, Loss: 1.6128654479980469, Accuracy: 0.521484375\n",
      "Batch: 45, Loss: 1.5392416715621948, Accuracy: 0.55078125\n",
      "Batch: 46, Loss: 1.7597744464874268, Accuracy: 0.5283203125\n",
      "Batch: 47, Loss: 1.7723326683044434, Accuracy: 0.5107421875\n",
      "Batch: 48, Loss: 1.7754435539245605, Accuracy: 0.51171875\n",
      "Batch: 49, Loss: 1.8479394912719727, Accuracy: 0.490234375\n",
      "Batch: 50, Loss: 1.7976677417755127, Accuracy: 0.48046875\n",
      "Batch: 51, Loss: 1.864107608795166, Accuracy: 0.4755859375\n",
      "Batch: 52, Loss: 1.8149209022521973, Accuracy: 0.490234375\n",
      "Batch: 53, Loss: 1.578643798828125, Accuracy: 0.5439453125\n",
      "Batch: 54, Loss: 1.6855002641677856, Accuracy: 0.5380859375\n",
      "Batch: 55, Loss: 1.6542680263519287, Accuracy: 0.5263671875\n",
      "Batch: 56, Loss: 1.8003278970718384, Accuracy: 0.4912109375\n",
      "Batch: 57, Loss: 1.703688621520996, Accuracy: 0.5244140625\n",
      "Batch: 58, Loss: 1.7640775442123413, Accuracy: 0.501953125\n",
      "Batch: 59, Loss: 1.5852750539779663, Accuracy: 0.556640625\n",
      "Batch: 60, Loss: 1.5945003032684326, Accuracy: 0.5439453125\n",
      "Batch: 61, Loss: 1.6967487335205078, Accuracy: 0.484375\n",
      "Batch: 62, Loss: 1.7413885593414307, Accuracy: 0.50390625\n",
      "Batch: 63, Loss: 1.7053570747375488, Accuracy: 0.5068359375\n",
      "Batch: 64, Loss: 1.6875650882720947, Accuracy: 0.509765625\n",
      "Batch: 65, Loss: 1.7124333381652832, Accuracy: 0.498046875\n",
      "Batch: 66, Loss: 1.5839955806732178, Accuracy: 0.5439453125\n",
      "Batch: 67, Loss: 1.711426019668579, Accuracy: 0.50390625\n",
      "Batch: 68, Loss: 1.7643787860870361, Accuracy: 0.5166015625\n",
      "Batch: 69, Loss: 1.7270143032073975, Accuracy: 0.533203125\n",
      "Batch: 70, Loss: 1.716759443283081, Accuracy: 0.5166015625\n",
      "Batch: 71, Loss: 1.6916847229003906, Accuracy: 0.515625\n",
      "Batch: 72, Loss: 1.5872124433517456, Accuracy: 0.5458984375\n",
      "Batch: 73, Loss: 1.6992833614349365, Accuracy: 0.5126953125\n",
      "Batch: 74, Loss: 1.5964374542236328, Accuracy: 0.5263671875\n",
      "Batch: 75, Loss: 1.5568492412567139, Accuracy: 0.53125\n",
      "Batch: 76, Loss: 1.650864601135254, Accuracy: 0.486328125\n",
      "Batch: 77, Loss: 1.699338674545288, Accuracy: 0.49609375\n",
      "Batch: 78, Loss: 1.6940327882766724, Accuracy: 0.5322265625\n",
      "Batch: 79, Loss: 1.5570244789123535, Accuracy: 0.595703125\n",
      "Batch: 80, Loss: 1.560842514038086, Accuracy: 0.525390625\n",
      "Batch: 81, Loss: 1.6739349365234375, Accuracy: 0.4912109375\n",
      "Batch: 82, Loss: 1.6567760705947876, Accuracy: 0.4990234375\n",
      "Batch: 83, Loss: 1.5239448547363281, Accuracy: 0.56640625\n",
      "Batch: 84, Loss: 1.6040987968444824, Accuracy: 0.5615234375\n",
      "Batch: 85, Loss: 1.58577299118042, Accuracy: 0.54296875\n",
      "Batch: 86, Loss: 1.7766804695129395, Accuracy: 0.4775390625\n",
      "Batch: 87, Loss: 1.5989902019500732, Accuracy: 0.5419921875\n",
      "Batch: 88, Loss: 1.703601360321045, Accuracy: 0.51171875\n",
      "Batch: 89, Loss: 1.7093571424484253, Accuracy: 0.5087890625\n",
      "Batch: 90, Loss: 1.5876426696777344, Accuracy: 0.533203125\n",
      "Batch: 91, Loss: 1.5875804424285889, Accuracy: 0.533203125\n",
      "Batch: 92, Loss: 1.6786834001541138, Accuracy: 0.509765625\n",
      "Batch: 93, Loss: 1.578288197517395, Accuracy: 0.529296875\n",
      "Batch: 94, Loss: 1.5949654579162598, Accuracy: 0.5244140625\n",
      "Batch: 95, Loss: 1.5952937602996826, Accuracy: 0.5166015625\n",
      "Batch: 96, Loss: 1.635840654373169, Accuracy: 0.541015625\n",
      "Batch: 97, Loss: 1.500277042388916, Accuracy: 0.5654296875\n",
      "Batch: 98, Loss: 1.5068082809448242, Accuracy: 0.5908203125\n",
      "Batch: 99, Loss: 1.540191650390625, Accuracy: 0.54296875\n",
      "Batch: 100, Loss: 1.5735833644866943, Accuracy: 0.5185546875\n",
      "Batch: 101, Loss: 1.6004815101623535, Accuracy: 0.525390625\n",
      "Batch: 102, Loss: 1.534555435180664, Accuracy: 0.5283203125\n",
      "Batch: 103, Loss: 1.657179594039917, Accuracy: 0.5517578125\n",
      "Batch: 104, Loss: 1.5199978351593018, Accuracy: 0.5341796875\n",
      "Batch: 105, Loss: 1.6266196966171265, Accuracy: 0.51171875\n",
      "Batch: 106, Loss: 1.6738932132720947, Accuracy: 0.5107421875\n",
      "Batch: 107, Loss: 1.7798080444335938, Accuracy: 0.4931640625\n",
      "Batch: 108, Loss: 1.7594144344329834, Accuracy: 0.5087890625\n",
      "Batch: 109, Loss: 1.7875972986221313, Accuracy: 0.4775390625\n",
      "Batch: 110, Loss: 1.4800077676773071, Accuracy: 0.572265625\n",
      "Batch: 111, Loss: 1.6122770309448242, Accuracy: 0.5029296875\n",
      "Batch: 112, Loss: 1.624563217163086, Accuracy: 0.53125\n",
      "Batch: 113, Loss: 1.6731330156326294, Accuracy: 0.5302734375\n",
      "Batch: 114, Loss: 1.6940655708312988, Accuracy: 0.4970703125\n",
      "Batch: 115, Loss: 1.7467955350875854, Accuracy: 0.5107421875\n",
      "Batch: 116, Loss: 1.7358055114746094, Accuracy: 0.47265625\n",
      "Batch: 117, Loss: 1.7169021368026733, Accuracy: 0.51953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 118, Loss: 1.5193297863006592, Accuracy: 0.5732421875\n",
      "Batch: 119, Loss: 1.5805888175964355, Accuracy: 0.5615234375\n",
      "Batch: 120, Loss: 1.6668641567230225, Accuracy: 0.5087890625\n",
      "Batch: 121, Loss: 1.7208971977233887, Accuracy: 0.4912109375\n",
      "Batch: 122, Loss: 1.6000341176986694, Accuracy: 0.5498046875\n",
      "Batch: 123, Loss: 1.5809786319732666, Accuracy: 0.5625\n",
      "Batch: 124, Loss: 1.6103386878967285, Accuracy: 0.5458984375\n",
      "Batch: 125, Loss: 1.6580562591552734, Accuracy: 0.515625\n",
      "Batch: 126, Loss: 1.649338960647583, Accuracy: 0.4931640625\n",
      "Batch: 127, Loss: 1.5367767810821533, Accuracy: 0.5673828125\n",
      "Batch: 128, Loss: 1.7625815868377686, Accuracy: 0.5029296875\n",
      "Batch: 129, Loss: 1.6100306510925293, Accuracy: 0.5302734375\n",
      "Batch: 130, Loss: 1.8294484615325928, Accuracy: 0.4736328125\n",
      "Batch: 131, Loss: 1.6639070510864258, Accuracy: 0.509765625\n",
      "Batch: 132, Loss: 1.7186518907546997, Accuracy: 0.5302734375\n",
      "Batch: 133, Loss: 1.6241557598114014, Accuracy: 0.5302734375\n",
      "Batch: 134, Loss: 1.6352674961090088, Accuracy: 0.521484375\n",
      "Batch: 135, Loss: 1.56361722946167, Accuracy: 0.5390625\n",
      "Batch: 136, Loss: 1.5809460878372192, Accuracy: 0.53515625\n",
      "Batch: 137, Loss: 1.4962905645370483, Accuracy: 0.5146484375\n",
      "Batch: 138, Loss: 1.385467290878296, Accuracy: 0.57421875\n",
      "Batch: 139, Loss: 1.4767693281173706, Accuracy: 0.546875\n",
      "Batch: 140, Loss: 1.5919495820999146, Accuracy: 0.525390625\n",
      "Batch: 141, Loss: 1.5671783685684204, Accuracy: 0.55078125\n",
      "Batch: 142, Loss: 1.603344440460205, Accuracy: 0.509765625\n",
      "Batch: 143, Loss: 1.634948492050171, Accuracy: 0.5146484375\n",
      "Batch: 144, Loss: 1.5842931270599365, Accuracy: 0.5419921875\n",
      "Batch: 145, Loss: 1.502124309539795, Accuracy: 0.546875\n",
      "Batch: 146, Loss: 1.6652891635894775, Accuracy: 0.49609375\n",
      "Batch: 147, Loss: 1.5648431777954102, Accuracy: 0.53515625\n",
      "Batch: 148, Loss: 1.7078502178192139, Accuracy: 0.474609375\n",
      "Batch: 149, Loss: 1.6316044330596924, Accuracy: 0.5224609375\n",
      "Batch: 150, Loss: 1.540791630744934, Accuracy: 0.5283203125\n",
      "Batch: 151, Loss: 1.5306694507598877, Accuracy: 0.55859375\n",
      "Epoch 4/80\n",
      "Batch: 1, Loss: 1.7264509201049805, Accuracy: 0.47265625\n",
      "Batch: 2, Loss: 1.51118803024292, Accuracy: 0.5068359375\n",
      "Batch: 3, Loss: 1.5078856945037842, Accuracy: 0.5390625\n",
      "Batch: 4, Loss: 1.454679250717163, Accuracy: 0.583984375\n",
      "Batch: 5, Loss: 1.5213454961776733, Accuracy: 0.5615234375\n",
      "Batch: 6, Loss: 1.5593369007110596, Accuracy: 0.521484375\n",
      "Batch: 7, Loss: 1.5198934078216553, Accuracy: 0.52734375\n",
      "Batch: 8, Loss: 1.457582950592041, Accuracy: 0.5517578125\n",
      "Batch: 9, Loss: 1.445039987564087, Accuracy: 0.55078125\n",
      "Batch: 10, Loss: 1.5082231760025024, Accuracy: 0.5361328125\n",
      "Batch: 11, Loss: 1.5873472690582275, Accuracy: 0.5048828125\n",
      "Batch: 12, Loss: 1.6566286087036133, Accuracy: 0.501953125\n",
      "Batch: 13, Loss: 1.364612102508545, Accuracy: 0.603515625\n",
      "Batch: 14, Loss: 1.6020431518554688, Accuracy: 0.5146484375\n",
      "Batch: 15, Loss: 1.5192310810089111, Accuracy: 0.5537109375\n",
      "Batch: 16, Loss: 1.4955787658691406, Accuracy: 0.53515625\n",
      "Batch: 17, Loss: 1.5528786182403564, Accuracy: 0.5146484375\n",
      "Batch: 18, Loss: 1.586219310760498, Accuracy: 0.5009765625\n",
      "Batch: 19, Loss: 1.6019856929779053, Accuracy: 0.5078125\n",
      "Batch: 20, Loss: 1.54657781124115, Accuracy: 0.5537109375\n",
      "Batch: 21, Loss: 1.4719574451446533, Accuracy: 0.5576171875\n",
      "Batch: 22, Loss: 1.651354432106018, Accuracy: 0.5234375\n",
      "Batch: 23, Loss: 1.4841194152832031, Accuracy: 0.546875\n",
      "Batch: 24, Loss: 1.569460391998291, Accuracy: 0.53125\n",
      "Batch: 25, Loss: 1.4865987300872803, Accuracy: 0.53515625\n",
      "Batch: 26, Loss: 1.4192912578582764, Accuracy: 0.564453125\n",
      "Batch: 27, Loss: 1.5234944820404053, Accuracy: 0.53125\n",
      "Batch: 28, Loss: 1.5318646430969238, Accuracy: 0.5126953125\n",
      "Batch: 29, Loss: 1.5660614967346191, Accuracy: 0.505859375\n",
      "Batch: 30, Loss: 1.5192041397094727, Accuracy: 0.55859375\n",
      "Batch: 31, Loss: 1.4984937906265259, Accuracy: 0.5673828125\n",
      "Batch: 32, Loss: 1.4419728517532349, Accuracy: 0.5556640625\n",
      "Batch: 33, Loss: 1.6806681156158447, Accuracy: 0.4970703125\n",
      "Batch: 34, Loss: 1.7228174209594727, Accuracy: 0.486328125\n",
      "Batch: 35, Loss: 1.5820732116699219, Accuracy: 0.521484375\n",
      "Batch: 36, Loss: 1.5541702508926392, Accuracy: 0.537109375\n",
      "Batch: 37, Loss: 1.5765082836151123, Accuracy: 0.5361328125\n",
      "Batch: 38, Loss: 1.5506079196929932, Accuracy: 0.5283203125\n",
      "Batch: 39, Loss: 1.5728363990783691, Accuracy: 0.537109375\n",
      "Batch: 40, Loss: 1.595662236213684, Accuracy: 0.5576171875\n",
      "Batch: 41, Loss: 1.643385887145996, Accuracy: 0.5234375\n",
      "Batch: 42, Loss: 1.3569445610046387, Accuracy: 0.59765625\n",
      "Batch: 43, Loss: 1.4913098812103271, Accuracy: 0.5302734375\n",
      "Batch: 44, Loss: 1.4456431865692139, Accuracy: 0.533203125\n",
      "Batch: 45, Loss: 1.349877953529358, Accuracy: 0.5751953125\n",
      "Batch: 46, Loss: 1.5549037456512451, Accuracy: 0.5556640625\n",
      "Batch: 47, Loss: 1.5678825378417969, Accuracy: 0.548828125\n",
      "Batch: 48, Loss: 1.5375876426696777, Accuracy: 0.546875\n",
      "Batch: 49, Loss: 1.6712744235992432, Accuracy: 0.494140625\n",
      "Batch: 50, Loss: 1.617336392402649, Accuracy: 0.521484375\n",
      "Batch: 51, Loss: 1.6721447706222534, Accuracy: 0.5087890625\n",
      "Batch: 52, Loss: 1.6293482780456543, Accuracy: 0.5146484375\n",
      "Batch: 53, Loss: 1.3923522233963013, Accuracy: 0.560546875\n",
      "Batch: 54, Loss: 1.5020359754562378, Accuracy: 0.572265625\n",
      "Batch: 55, Loss: 1.4928725957870483, Accuracy: 0.5380859375\n",
      "Batch: 56, Loss: 1.6197088956832886, Accuracy: 0.51953125\n",
      "Batch: 57, Loss: 1.5137966871261597, Accuracy: 0.560546875\n",
      "Batch: 58, Loss: 1.6196014881134033, Accuracy: 0.521484375\n",
      "Batch: 59, Loss: 1.4014075994491577, Accuracy: 0.5986328125\n",
      "Batch: 60, Loss: 1.4177167415618896, Accuracy: 0.576171875\n",
      "Batch: 61, Loss: 1.5267150402069092, Accuracy: 0.5341796875\n",
      "Batch: 62, Loss: 1.5530855655670166, Accuracy: 0.5546875\n",
      "Batch: 63, Loss: 1.544083833694458, Accuracy: 0.51953125\n",
      "Batch: 64, Loss: 1.4830906391143799, Accuracy: 0.5419921875\n",
      "Batch: 65, Loss: 1.517423391342163, Accuracy: 0.5400390625\n",
      "Batch: 66, Loss: 1.4037559032440186, Accuracy: 0.58984375\n",
      "Batch: 67, Loss: 1.6018807888031006, Accuracy: 0.5185546875\n",
      "Batch: 68, Loss: 1.6122344732284546, Accuracy: 0.5537109375\n",
      "Batch: 69, Loss: 1.5394349098205566, Accuracy: 0.537109375\n",
      "Batch: 70, Loss: 1.527450442314148, Accuracy: 0.5498046875\n",
      "Batch: 71, Loss: 1.5242571830749512, Accuracy: 0.53515625\n",
      "Batch: 72, Loss: 1.433605432510376, Accuracy: 0.5556640625\n",
      "Batch: 73, Loss: 1.501082420349121, Accuracy: 0.5576171875\n",
      "Batch: 74, Loss: 1.4361236095428467, Accuracy: 0.556640625\n",
      "Batch: 75, Loss: 1.3930634260177612, Accuracy: 0.5673828125\n",
      "Batch: 76, Loss: 1.508528470993042, Accuracy: 0.51171875\n",
      "Batch: 77, Loss: 1.5209769010543823, Accuracy: 0.517578125\n",
      "Batch: 78, Loss: 1.5269279479980469, Accuracy: 0.5576171875\n",
      "Batch: 79, Loss: 1.3679590225219727, Accuracy: 0.6171875\n",
      "Batch: 80, Loss: 1.3840150833129883, Accuracy: 0.57421875\n",
      "Batch: 81, Loss: 1.545832872390747, Accuracy: 0.484375\n",
      "Batch: 82, Loss: 1.5031681060791016, Accuracy: 0.521484375\n",
      "Batch: 83, Loss: 1.3608291149139404, Accuracy: 0.6015625\n",
      "Batch: 84, Loss: 1.4333915710449219, Accuracy: 0.5849609375\n",
      "Batch: 85, Loss: 1.394869089126587, Accuracy: 0.5732421875\n",
      "Batch: 86, Loss: 1.6318089962005615, Accuracy: 0.5\n",
      "Batch: 87, Loss: 1.425367832183838, Accuracy: 0.572265625\n",
      "Batch: 88, Loss: 1.5453085899353027, Accuracy: 0.5419921875\n",
      "Batch: 89, Loss: 1.5502146482467651, Accuracy: 0.51953125\n",
      "Batch: 90, Loss: 1.4133458137512207, Accuracy: 0.560546875\n",
      "Batch: 91, Loss: 1.4412806034088135, Accuracy: 0.5517578125\n",
      "Batch: 92, Loss: 1.5393167734146118, Accuracy: 0.533203125\n",
      "Batch: 93, Loss: 1.4215583801269531, Accuracy: 0.5615234375\n",
      "Batch: 94, Loss: 1.4438267946243286, Accuracy: 0.5537109375\n",
      "Batch: 95, Loss: 1.482673168182373, Accuracy: 0.5048828125\n",
      "Batch: 96, Loss: 1.471817970275879, Accuracy: 0.5498046875\n",
      "Batch: 97, Loss: 1.3552944660186768, Accuracy: 0.595703125\n",
      "Batch: 98, Loss: 1.363694667816162, Accuracy: 0.599609375\n",
      "Batch: 99, Loss: 1.35676908493042, Accuracy: 0.5966796875\n",
      "Batch: 100, Loss: 1.442726492881775, Accuracy: 0.5478515625\n",
      "Batch: 101, Loss: 1.4666917324066162, Accuracy: 0.5400390625\n",
      "Batch: 102, Loss: 1.3756179809570312, Accuracy: 0.576171875\n",
      "Batch: 103, Loss: 1.4905624389648438, Accuracy: 0.5732421875\n",
      "Batch: 104, Loss: 1.384475588798523, Accuracy: 0.556640625\n",
      "Batch: 105, Loss: 1.4948574304580688, Accuracy: 0.5439453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 106, Loss: 1.5289676189422607, Accuracy: 0.5283203125\n",
      "Batch: 107, Loss: 1.6356877088546753, Accuracy: 0.5166015625\n",
      "Batch: 108, Loss: 1.5782313346862793, Accuracy: 0.5439453125\n",
      "Batch: 109, Loss: 1.6269041299819946, Accuracy: 0.505859375\n",
      "Batch: 110, Loss: 1.3230836391448975, Accuracy: 0.5810546875\n",
      "Batch: 111, Loss: 1.4979180097579956, Accuracy: 0.5224609375\n",
      "Batch: 112, Loss: 1.4887721538543701, Accuracy: 0.5537109375\n",
      "Batch: 113, Loss: 1.5080269575119019, Accuracy: 0.576171875\n",
      "Batch: 114, Loss: 1.572899341583252, Accuracy: 0.51953125\n",
      "Batch: 115, Loss: 1.6234164237976074, Accuracy: 0.53515625\n",
      "Batch: 116, Loss: 1.5752021074295044, Accuracy: 0.5224609375\n",
      "Batch: 117, Loss: 1.591491460800171, Accuracy: 0.54296875\n",
      "Batch: 118, Loss: 1.3498282432556152, Accuracy: 0.59375\n",
      "Batch: 119, Loss: 1.4069349765777588, Accuracy: 0.58984375\n",
      "Batch: 120, Loss: 1.5624773502349854, Accuracy: 0.5107421875\n",
      "Batch: 121, Loss: 1.6026251316070557, Accuracy: 0.5126953125\n",
      "Batch: 122, Loss: 1.447351336479187, Accuracy: 0.5654296875\n",
      "Batch: 123, Loss: 1.4402129650115967, Accuracy: 0.572265625\n",
      "Batch: 124, Loss: 1.5004481077194214, Accuracy: 0.55859375\n",
      "Batch: 125, Loss: 1.5672802925109863, Accuracy: 0.5234375\n",
      "Batch: 126, Loss: 1.5356932878494263, Accuracy: 0.5185546875\n",
      "Batch: 127, Loss: 1.3912804126739502, Accuracy: 0.5966796875\n",
      "Batch: 128, Loss: 1.6361085176467896, Accuracy: 0.5341796875\n",
      "Batch: 129, Loss: 1.4687683582305908, Accuracy: 0.5576171875\n",
      "Batch: 130, Loss: 1.721412181854248, Accuracy: 0.5029296875\n",
      "Batch: 131, Loss: 1.5501067638397217, Accuracy: 0.5322265625\n",
      "Batch: 132, Loss: 1.587296724319458, Accuracy: 0.515625\n",
      "Batch: 133, Loss: 1.4321849346160889, Accuracy: 0.5634765625\n",
      "Batch: 134, Loss: 1.4808343648910522, Accuracy: 0.548828125\n",
      "Batch: 135, Loss: 1.435197114944458, Accuracy: 0.5537109375\n",
      "Batch: 136, Loss: 1.445157766342163, Accuracy: 0.5556640625\n",
      "Batch: 137, Loss: 1.4202622175216675, Accuracy: 0.533203125\n",
      "Batch: 138, Loss: 1.2844456434249878, Accuracy: 0.576171875\n",
      "Batch: 139, Loss: 1.3475590944290161, Accuracy: 0.5712890625\n",
      "Batch: 140, Loss: 1.4732246398925781, Accuracy: 0.5400390625\n",
      "Batch: 141, Loss: 1.4675014019012451, Accuracy: 0.55859375\n",
      "Batch: 142, Loss: 1.494774580001831, Accuracy: 0.5322265625\n",
      "Batch: 143, Loss: 1.501285195350647, Accuracy: 0.5458984375\n",
      "Batch: 144, Loss: 1.4715032577514648, Accuracy: 0.556640625\n",
      "Batch: 145, Loss: 1.4028879404067993, Accuracy: 0.5712890625\n",
      "Batch: 146, Loss: 1.543239951133728, Accuracy: 0.5244140625\n",
      "Batch: 147, Loss: 1.4542756080627441, Accuracy: 0.5537109375\n",
      "Batch: 148, Loss: 1.6238287687301636, Accuracy: 0.5029296875\n",
      "Batch: 149, Loss: 1.5009143352508545, Accuracy: 0.5302734375\n",
      "Batch: 150, Loss: 1.4046008586883545, Accuracy: 0.54296875\n",
      "Batch: 151, Loss: 1.4144527912139893, Accuracy: 0.583984375\n",
      "Epoch 5/80\n",
      "Batch: 1, Loss: 1.6663340330123901, Accuracy: 0.46484375\n",
      "Batch: 2, Loss: 1.4355709552764893, Accuracy: 0.5224609375\n",
      "Batch: 3, Loss: 1.381989598274231, Accuracy: 0.5771484375\n",
      "Batch: 4, Loss: 1.3358047008514404, Accuracy: 0.609375\n",
      "Batch: 5, Loss: 1.4062402248382568, Accuracy: 0.568359375\n",
      "Batch: 6, Loss: 1.5027292966842651, Accuracy: 0.5263671875\n",
      "Batch: 7, Loss: 1.4340696334838867, Accuracy: 0.5478515625\n",
      "Batch: 8, Loss: 1.3528169393539429, Accuracy: 0.57421875\n",
      "Batch: 9, Loss: 1.3202465772628784, Accuracy: 0.607421875\n",
      "Batch: 10, Loss: 1.3993662595748901, Accuracy: 0.564453125\n",
      "Batch: 11, Loss: 1.5023177862167358, Accuracy: 0.521484375\n",
      "Batch: 12, Loss: 1.5781280994415283, Accuracy: 0.5146484375\n",
      "Batch: 13, Loss: 1.2619435787200928, Accuracy: 0.611328125\n",
      "Batch: 14, Loss: 1.5179595947265625, Accuracy: 0.5126953125\n",
      "Batch: 15, Loss: 1.4090479612350464, Accuracy: 0.5830078125\n",
      "Batch: 16, Loss: 1.3962860107421875, Accuracy: 0.556640625\n",
      "Batch: 17, Loss: 1.4596130847930908, Accuracy: 0.5439453125\n",
      "Batch: 18, Loss: 1.5221843719482422, Accuracy: 0.5302734375\n",
      "Batch: 19, Loss: 1.5352091789245605, Accuracy: 0.521484375\n",
      "Batch: 20, Loss: 1.4531993865966797, Accuracy: 0.5732421875\n",
      "Batch: 21, Loss: 1.3763253688812256, Accuracy: 0.564453125\n",
      "Batch: 22, Loss: 1.5412929058074951, Accuracy: 0.5244140625\n",
      "Batch: 23, Loss: 1.3745949268341064, Accuracy: 0.568359375\n",
      "Batch: 24, Loss: 1.4867602586746216, Accuracy: 0.55859375\n",
      "Batch: 25, Loss: 1.4090685844421387, Accuracy: 0.55859375\n",
      "Batch: 26, Loss: 1.3192417621612549, Accuracy: 0.5966796875\n",
      "Batch: 27, Loss: 1.4140628576278687, Accuracy: 0.556640625\n",
      "Batch: 28, Loss: 1.4621267318725586, Accuracy: 0.5498046875\n",
      "Batch: 29, Loss: 1.4768857955932617, Accuracy: 0.5126953125\n",
      "Batch: 30, Loss: 1.4451422691345215, Accuracy: 0.572265625\n",
      "Batch: 31, Loss: 1.3678438663482666, Accuracy: 0.5986328125\n",
      "Batch: 32, Loss: 1.33322012424469, Accuracy: 0.5771484375\n",
      "Batch: 33, Loss: 1.576277732849121, Accuracy: 0.5224609375\n",
      "Batch: 34, Loss: 1.6406669616699219, Accuracy: 0.4951171875\n",
      "Batch: 35, Loss: 1.4633452892303467, Accuracy: 0.5439453125\n",
      "Batch: 36, Loss: 1.4826908111572266, Accuracy: 0.548828125\n",
      "Batch: 37, Loss: 1.4871995449066162, Accuracy: 0.54296875\n",
      "Batch: 38, Loss: 1.4590256214141846, Accuracy: 0.5478515625\n",
      "Batch: 39, Loss: 1.4776124954223633, Accuracy: 0.5517578125\n",
      "Batch: 40, Loss: 1.4860788583755493, Accuracy: 0.57421875\n",
      "Batch: 41, Loss: 1.5515097379684448, Accuracy: 0.5419921875\n",
      "Batch: 42, Loss: 1.2596246004104614, Accuracy: 0.5986328125\n",
      "Batch: 43, Loss: 1.400620937347412, Accuracy: 0.546875\n",
      "Batch: 44, Loss: 1.3796911239624023, Accuracy: 0.5439453125\n",
      "Batch: 45, Loss: 1.2691593170166016, Accuracy: 0.580078125\n",
      "Batch: 46, Loss: 1.4654065370559692, Accuracy: 0.5810546875\n",
      "Batch: 47, Loss: 1.462183952331543, Accuracy: 0.5732421875\n",
      "Batch: 48, Loss: 1.415989875793457, Accuracy: 0.58203125\n",
      "Batch: 49, Loss: 1.5548242330551147, Accuracy: 0.52734375\n",
      "Batch: 50, Loss: 1.513260006904602, Accuracy: 0.5244140625\n",
      "Batch: 51, Loss: 1.5928547382354736, Accuracy: 0.51171875\n",
      "Batch: 52, Loss: 1.5422571897506714, Accuracy: 0.5419921875\n",
      "Batch: 53, Loss: 1.307093858718872, Accuracy: 0.5947265625\n",
      "Batch: 54, Loss: 1.4137682914733887, Accuracy: 0.5830078125\n",
      "Batch: 55, Loss: 1.4167494773864746, Accuracy: 0.5478515625\n",
      "Batch: 56, Loss: 1.5146147012710571, Accuracy: 0.5458984375\n",
      "Batch: 57, Loss: 1.4398832321166992, Accuracy: 0.5771484375\n",
      "Batch: 58, Loss: 1.5426759719848633, Accuracy: 0.5322265625\n",
      "Batch: 59, Loss: 1.3135502338409424, Accuracy: 0.611328125\n",
      "Batch: 60, Loss: 1.3286340236663818, Accuracy: 0.5830078125\n",
      "Batch: 61, Loss: 1.441292643547058, Accuracy: 0.5439453125\n",
      "Batch: 62, Loss: 1.4296519756317139, Accuracy: 0.5615234375\n",
      "Batch: 63, Loss: 1.429087519645691, Accuracy: 0.5478515625\n"
     ]
    }
   ],
   "source": [
    "file = open(os.path.join(data_directory, data_file), mode = 'r')\n",
    "data = file.read()\n",
    "file.close()\n",
    "if __name__ == \"__main__\":\n",
    "    training_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.643317</td>\n",
       "      <td>0.290039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.873376</td>\n",
       "      <td>0.496094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.548782</td>\n",
       "      <td>0.557617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.417467</td>\n",
       "      <td>0.597656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.348234</td>\n",
       "      <td>0.585938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1.265394</td>\n",
       "      <td>0.618164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1.186394</td>\n",
       "      <td>0.630859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1.145774</td>\n",
       "      <td>0.642578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1.097427</td>\n",
       "      <td>0.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1.073594</td>\n",
       "      <td>0.650391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1.052364</td>\n",
       "      <td>0.674805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1.011208</td>\n",
       "      <td>0.666016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1.004766</td>\n",
       "      <td>0.672852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.980474</td>\n",
       "      <td>0.685547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.972413</td>\n",
       "      <td>0.681641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.935533</td>\n",
       "      <td>0.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.930730</td>\n",
       "      <td>0.695312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.896762</td>\n",
       "      <td>0.713867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.896243</td>\n",
       "      <td>0.708008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.886423</td>\n",
       "      <td>0.713867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.853774</td>\n",
       "      <td>0.722656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.858230</td>\n",
       "      <td>0.716797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.861040</td>\n",
       "      <td>0.707031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.835705</td>\n",
       "      <td>0.721680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.818098</td>\n",
       "      <td>0.735352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0.807396</td>\n",
       "      <td>0.725586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.800719</td>\n",
       "      <td>0.735352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.797581</td>\n",
       "      <td>0.740234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0.786037</td>\n",
       "      <td>0.737305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0.769117</td>\n",
       "      <td>0.744141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>51</td>\n",
       "      <td>0.649517</td>\n",
       "      <td>0.786133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>52</td>\n",
       "      <td>0.646988</td>\n",
       "      <td>0.793945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>53</td>\n",
       "      <td>0.635747</td>\n",
       "      <td>0.794922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>54</td>\n",
       "      <td>0.626719</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>55</td>\n",
       "      <td>0.643305</td>\n",
       "      <td>0.793945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>56</td>\n",
       "      <td>0.628394</td>\n",
       "      <td>0.803711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>57</td>\n",
       "      <td>0.639661</td>\n",
       "      <td>0.797852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>58</td>\n",
       "      <td>0.620944</td>\n",
       "      <td>0.800781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>59</td>\n",
       "      <td>0.593881</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>60</td>\n",
       "      <td>0.609697</td>\n",
       "      <td>0.798828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>61</td>\n",
       "      <td>0.592073</td>\n",
       "      <td>0.807617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>62</td>\n",
       "      <td>0.591695</td>\n",
       "      <td>0.807617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>63</td>\n",
       "      <td>0.593241</td>\n",
       "      <td>0.805664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>0.592210</td>\n",
       "      <td>0.815430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>0.600351</td>\n",
       "      <td>0.801758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>0.546197</td>\n",
       "      <td>0.823242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>0.582400</td>\n",
       "      <td>0.817383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>0.582353</td>\n",
       "      <td>0.817383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>0.560872</td>\n",
       "      <td>0.809570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>70</td>\n",
       "      <td>0.562345</td>\n",
       "      <td>0.805664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>0.562496</td>\n",
       "      <td>0.828125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>0.558382</td>\n",
       "      <td>0.818359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73</td>\n",
       "      <td>0.558365</td>\n",
       "      <td>0.821289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>0.576193</td>\n",
       "      <td>0.820312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>75</td>\n",
       "      <td>0.592619</td>\n",
       "      <td>0.817383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>0.537521</td>\n",
       "      <td>0.838867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>0.558197</td>\n",
       "      <td>0.821289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>0.541944</td>\n",
       "      <td>0.835938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>0.534475</td>\n",
       "      <td>0.825195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>0.515541</td>\n",
       "      <td>0.828125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch      Loss  Accuracy\n",
       "0       1  2.643317  0.290039\n",
       "1       2  1.873376  0.496094\n",
       "2       3  1.548782  0.557617\n",
       "3       4  1.417467  0.597656\n",
       "4       5  1.348234  0.585938\n",
       "5       6  1.265394  0.618164\n",
       "6       7  1.186394  0.630859\n",
       "7       8  1.145774  0.642578\n",
       "8       9  1.097427  0.656250\n",
       "9      10  1.073594  0.650391\n",
       "10     11  1.052364  0.674805\n",
       "11     12  1.011208  0.666016\n",
       "12     13  1.004766  0.672852\n",
       "13     14  0.980474  0.685547\n",
       "14     15  0.972413  0.681641\n",
       "15     16  0.935533  0.703125\n",
       "16     17  0.930730  0.695312\n",
       "17     18  0.896762  0.713867\n",
       "18     19  0.896243  0.708008\n",
       "19     20  0.886423  0.713867\n",
       "20     21  0.853774  0.722656\n",
       "21     22  0.858230  0.716797\n",
       "22     23  0.861040  0.707031\n",
       "23     24  0.835705  0.721680\n",
       "24     25  0.818098  0.735352\n",
       "25     26  0.807396  0.725586\n",
       "26     27  0.800719  0.735352\n",
       "27     28  0.797581  0.740234\n",
       "28     29  0.786037  0.737305\n",
       "29     30  0.769117  0.744141\n",
       "..    ...       ...       ...\n",
       "50     51  0.649517  0.786133\n",
       "51     52  0.646988  0.793945\n",
       "52     53  0.635747  0.794922\n",
       "53     54  0.626719  0.812500\n",
       "54     55  0.643305  0.793945\n",
       "55     56  0.628394  0.803711\n",
       "56     57  0.639661  0.797852\n",
       "57     58  0.620944  0.800781\n",
       "58     59  0.593881  0.812500\n",
       "59     60  0.609697  0.798828\n",
       "60     61  0.592073  0.807617\n",
       "61     62  0.591695  0.807617\n",
       "62     63  0.593241  0.805664\n",
       "63     64  0.592210  0.815430\n",
       "64     65  0.600351  0.801758\n",
       "65     66  0.546197  0.823242\n",
       "66     67  0.582400  0.817383\n",
       "67     68  0.582353  0.817383\n",
       "68     69  0.560872  0.809570\n",
       "69     70  0.562345  0.805664\n",
       "70     71  0.562496  0.828125\n",
       "71     72  0.558382  0.818359\n",
       "72     73  0.558365  0.821289\n",
       "73     74  0.576193  0.820312\n",
       "74     75  0.592619  0.817383\n",
       "75     76  0.537521  0.838867\n",
       "76     77  0.558197  0.821289\n",
       "77     78  0.541944  0.835938\n",
       "78     79  0.534475  0.825195\n",
       "79     80  0.515541  0.828125\n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = pd.read_csv(os.path.join(data_directory, \"log.csv\"))\n",
    "log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
